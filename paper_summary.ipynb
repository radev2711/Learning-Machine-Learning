{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiating Concepts and Instances for Knowledge Graph Embedding\n",
    "\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, pnorm=0):\n",
    "    if pnorm == 1:\n",
    "        return torch.sum(torch.abs(x), -1)\n",
    "    else:\n",
    "        return torch.sum(x**2,-1)\n",
    "\n",
    "def normalize_emb(x):\n",
    "    # return  x/float(length)\n",
    "    veclen = torch.clamp_min_(torch.norm(x, 2, -1,keepdim=True), 1.0)\n",
    "    ret = x/veclen\n",
    "    return ret.detach()\n",
    "\n",
    "def normalize_radius(x):\n",
    "    return torch.clamp(x,min=-1.0,max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, args):\n",
    "        self.dataset_name = args[\"dataset\"]\n",
    "        self.args = args\n",
    "        self.entity_num, self.entity2id = self.read_file(self.dataset_name, \"instance2id\")\n",
    "        self.relation_num, self.relation2id = self.read_file(self.dataset_name, \"relation2id\")\n",
    "        self.concept_num, self.concept2id = self.read_file(self.dataset_name, \"concept2id\")\n",
    "        self.triple_num, self.triples = self.read_triples(self.dataset_name, \"triple2id\")\n",
    "\n",
    "        self.fb_h, self.fb_t, self.fb_r = [], [], []\n",
    "        self.relation_vec,self.entity_vec,self.concept_vec = [],[],[]\n",
    "        self.relation_tmp, self.entity_tmp, self.concept_tmp = [], [], []\n",
    "        self.concept_r, self.concept_r_tmp = [], []\n",
    "        self.ok = {}\n",
    "        self.subClassOf_ok = {}\n",
    "        self.instanceOf_ok = {}\n",
    "        self.subClassOf = []\n",
    "        self.instanceOf = []\n",
    "        self.instance_concept = [[] for i in range(self.entity_num)]\n",
    "        self.concept_instance = [[] for i in range(self.concept_num)]\n",
    "        self.sub_up_concept = [[] for i in range(self.concept_num)]\n",
    "        self.up_sub_concept = [[] for i in range(self.concept_num)]\n",
    "\n",
    "\n",
    "    def read_file(self, dataset,filename,split = 'Train'):\n",
    "        with open(\"data/\" + dataset + \"/\" + split+\"/\"+filename + \".txt\") as file:\n",
    "            L = file.readlines()\n",
    "            num = int(L[0].strip())\n",
    "            contents = [[x for x in line.strip().split()] for line in L[1:]]\n",
    "        return num, contents\n",
    "\n",
    "    def read_triples(self, dataset,filename,split = 'Train'):\n",
    "        with open(\"data/\" + dataset + \"/\" + split+\"/\"+filename + \".txt\") as file:\n",
    "            L = file.readlines()\n",
    "            num = int(L[0].strip())\n",
    "            contents = [[int(x) for x in line.strip().split()] for line in L[1:]]\n",
    "        return num, contents\n",
    "\n",
    "    def read_biples(self, dataset, filename,split = 'Train'):\n",
    "        with open(\"data/\" + dataset + \"/\" + split+\"/\"+filename + \".txt\") as file:\n",
    "            L = file.readlines()\n",
    "            contents = [[int(x) for x in line.strip().split()] for line in L[1:]]\n",
    "        return contents\n",
    "\n",
    "    def addHrt(self, x, y, z):  # x: head ,y: tail, z:relation\n",
    "        self.fb_h.append(x)\n",
    "        self.fb_r.append(z)\n",
    "        self.fb_t.append(y)\n",
    "        if (x, z) not in self.ok:\n",
    "            self.ok[(x, z)] = {y: 1}\n",
    "        else:\n",
    "            self.ok[(x, z)][y] = 1\n",
    "\n",
    "    def addSubClassOf(self, sub, parent):\n",
    "        self.subClassOf.append([sub, parent])\n",
    "        self.subClassOf_ok[(sub, parent)] = 1\n",
    "\n",
    "    def addInstanceOf(self, instance, concept):\n",
    "        self.instanceOf.append([instance, concept])\n",
    "        self.instanceOf_ok[(instance, concept)] = 1\n",
    "\n",
    "    def setup(self):\n",
    "        self.left_entity = [Counter() for i in range(self.relation_num)]\n",
    "        self.right_entity = [Counter() for i in range(self.relation_num)]\n",
    "\n",
    "        for h, t, r in self.triples:\n",
    "            self.addHrt(h, t, r)\n",
    "            if self.args[\"bern\"]:\n",
    "                self.left_entity[r][h] += 1\n",
    "                self.right_entity[r][t] += 1\n",
    "\n",
    "        self.left_num = [float(sum(c.values())) / float(len(c)) for c in self.left_entity]\n",
    "        self.right_num = [float(sum(c.values())) / float(len(c)) for c in self.right_entity]\n",
    "\n",
    "        self.instanceOf_contents = self.read_biples(self.args[\"dataset\"], \"instanceOf2id\")\n",
    "        self.subClassOf_contents = self.read_biples(self.args[\"dataset\"], \"subClassOf2id\")\n",
    "\n",
    "        for a, b in self.instanceOf_contents:\n",
    "            self.addInstanceOf(a,b)\n",
    "            self.instance_concept[a].append(b)\n",
    "            self.concept_instance[b].append(a)\n",
    "\n",
    "        for a, b in self.subClassOf_contents:\n",
    "            self.addSubClassOf(a,b)\n",
    "            self.sub_up_concept[a].append(b)\n",
    "            self.up_sub_concept[b].append(a)\n",
    "\n",
    "\n",
    "        self.instance_brother = [[ins for concept in concepts\n",
    "                                for ins in self.concept_instance[concept]\n",
    "                                if ins != instance_out]\n",
    "                                for instance_out, concepts\n",
    "                                in enumerate(self.instance_concept)]\n",
    "\n",
    "        self.concept_brother = [[sub for up in ups\n",
    "                                for sub in self.up_sub_concept[up]\n",
    "                                if sub != sub_out]\n",
    "                                for sub_out, ups\n",
    "                                in enumerate(self.sub_up_concept)]\n",
    "\n",
    "        self.trainSize = len(self.fb_h) + len(self.instanceOf) + len(self.subClassOf)\n",
    "\n",
    "        print(\"train size {} {} {} {}\".format(self.trainSize, len(self.fb_h),len(self.instanceOf),len(self.subClassOf)))\n",
    "\n",
    "    def save(self,):\n",
    "        with open(\"data/\" + self.dataset_name + \"/\" + self.args[\"split\"] + \"/processed.pkl\",'wb') as file:\n",
    "            pkl.dump(self, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed(dataset_name,split):\n",
    "    with open(\"data/\" + dataset_name + \"/\" + split + \"/processed.pkl\",'rb') as file:\n",
    "        res = pkl.load(file)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(nn.Module):\n",
    "    def __init__(self,args,dataset):\n",
    "        super(Train, self).__init__()\n",
    "        self.args = args\n",
    "        self.D = dataset\n",
    "        self.entity_vec = nn.Embedding(self.D.entity_num,args[\"emb_dim\"])\n",
    "        self.concept_vec = nn.Embedding(self.D.concept_num,args[\"emb_dim\"]+1)\n",
    "        self.relation_vec = nn.Embedding(self.D.relation_num,args[\"emb_dim\"])\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(),lr=args[\"lr\"])\n",
    "\n",
    "        ## initialize\n",
    "        nn.init.normal_(self.entity_vec.weight.data, 0.0, 1.0 / args[\"emb_dim\"])\n",
    "        nn.init.normal_(self.relation_vec.weight.data, 0.0, 1.0 / args[\"emb_dim\"])\n",
    "        nn.init.normal_(self.concept_vec.weight.data[:, :-1], 0.0, 1.0 / args[\"emb_dim\"])\n",
    "        nn.init.uniform_(self.concept_vec.weight.data[:, -1], 0.0, 1.0)\n",
    "\n",
    "        # self.training_instance_file = open(\"data/cpp_training_instance.txt\", 'r')\n",
    "        # with open(\"data/cpp_training_instance.txt\", 'r') as file:\n",
    "        #     lines = file.readlines()\n",
    "        #     lines = [line.strip().split(\"\\t\") for line in lines]\n",
    "        #     self.training_instance = [[int(x) for x in line] for line in lines ]\n",
    "        #     print(\"using saved instances\")\n",
    "\n",
    "    def doTrain(self):\n",
    "        nbatches = self.args[\"nbatches\"]\n",
    "        nepoch = self.args[\"nepoch\"]\n",
    "        batchSize = int(self.D.trainSize / nbatches)\n",
    "        allreadyindex = 0\n",
    "\n",
    "        dis_a_L, dis_b_L = [], []\n",
    "        dis_count = 0\n",
    "        for epoch in range(nepoch):\n",
    "            res = 0\n",
    "            for batch in range(nbatches):\n",
    "                losses = []\n",
    "                stime = time.time()\n",
    "                pairs = [[], [], []]\n",
    "\n",
    "                #normalize\n",
    "                self.entity_vec.weight.data = normalize_emb(self.entity_vec.weight.data)\n",
    "                self.relation_vec.weight.data = normalize_emb(self.relation_vec.weight.data)\n",
    "                self.concept_vec.weight.data[:, :-1] = normalize_emb(self.concept_vec.weight.data[:, :-1])\n",
    "                self.concept_vec.weight.data[:, -1] = normalize_radius(self.concept_vec.weight.data[:, -1])\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                for k in range(batchSize):\n",
    "                    i = random.randint(0, self.D.trainSize - 1)\n",
    "                    if i < len(self.D.fb_r):\n",
    "                        cut = 1 - epoch * self.args[\"hrt_cut\"] / nepoch\n",
    "                        pairs[0].append(self.trainHLR(i, cut))\n",
    "                    elif i < len(self.D.fb_r) + len(self.D.instanceOf):\n",
    "                        cut = 1 - epoch * self.args[\"ins_cut\"] / nepoch\n",
    "                        pairs[1].append(self.trainInstanceOf(i, cut))\n",
    "                    else:\n",
    "                        cut = 1 - epoch * self.args[\"sub_cut\"] / nepoch\n",
    "                        pairs[2].append(self.trainSubClassOf(i, cut))\n",
    "\n",
    "                # for k in range(batchSize):\n",
    "                #     line = self.training_instance_file.readline()\n",
    "                #     line = line.strip().split(\"\\t\")\n",
    "                #     instance = [int(x) for x in line]\n",
    "                #     # print(instance)\n",
    "                #     if instance[0] == -1:\n",
    "                #         pairs[0].append(instance[1:])\n",
    "                #     if instance[0] == -2:\n",
    "                #         pairs[1].append(instance[1:])\n",
    "                #     if instance[0] == -3:\n",
    "                #         pairs[2].append(instance[1:])\n",
    "                # allreadyindex += batchSize\n",
    "\n",
    "                tensor_pairs= []\n",
    "                for i in range(3):\n",
    "                    tensor_pairs.append(torch.stack([torch.tensor(x) for x in list(zip(*pairs[i]))]).cuda())\n",
    "                loss1,dis_a,dis_b = self.doTrainHLR(tensor_pairs[0])\n",
    "                loss2 = self.doTrainInstanceOf(tensor_pairs[1])\n",
    "                loss3 = self.doTrainSubClassOf(tensor_pairs[2])\n",
    "                losses = loss1 + loss2 + loss3\n",
    "                losses.backward()\n",
    "\n",
    "                dis_a_L.append(torch.sqrt(dis_a).sum()), dis_b_L.append(torch.sqrt(dis_b).sum()) # for logs\n",
    "                dis_count += dis_a.size(0)\n",
    "\n",
    "                self.optimizer.step()\n",
    "                res += losses.detach().cpu().numpy()\n",
    "\n",
    "            print(sum(dis_a_L) / dis_count, sum(dis_b_L) / dis_count, dis_a.size())\n",
    "            dis_a_L, dis_b_L = [], []\n",
    "            dis_count = 0\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"epoch:{} Res: {:.6f} Loss {:.6f},loss1: {:.6f},loss2: {:.6f},loss3 {:.6f}\".format(epoch,res,losses,loss1,loss2,loss3))\n",
    "            if epoch % 500 == 0 or epoch == nepoch - 1:\n",
    "                entity_vec_save = self.entity_vec.weight.detach().cpu().numpy()\n",
    "                concept_vec_save = self.concept_vec.weight.detach().cpu().numpy()\n",
    "                relation_vec_save = self.relation_vec.weight.detach().cpu().numpy()\n",
    "\n",
    "                # with open(\"embeddings/transc/\"+self.args.version+\"_embeddings_epoch\" + str(epoch) + \".pkl\", 'wb') as file:\n",
    "                #    pkl.dump({\"entity_vec\": entity_vec_save,\n",
    "                #             \"concept_vec\": concept_vec_save,\n",
    "                #             \"relation_vec\":relation_vec_save},file)\n",
    "                #print(\"saved!\")\n",
    "\n",
    "                #write for cpp test\n",
    "                with open(\"vector/\"+self.args[\"dataset\"] +\"/entity2vec.vec\", 'w') as file:\n",
    "                    for vec in entity_vec_save:\n",
    "                        list_vec = list(vec)\n",
    "                        str_vec = \"\\t\".join([str(x) for x in list_vec])\n",
    "                        file.write(str_vec+\"\\n\")\n",
    "\n",
    "                with open(\"vector/\"+ self.args[\"dataset\"] + \"/relation2vec.vec\", 'w') as file:\n",
    "                    for vec in relation_vec_save:\n",
    "                        list_vec = list(vec)\n",
    "                        str_vec = \"\\t\".join([str(x) for x in list_vec])\n",
    "                        file.write(str_vec+\"\\n\")\n",
    "\n",
    "                with open(\"vector/\" + self.args[\"dataset\"]+\"/concept2vec.vec\", 'w') as file:\n",
    "                    for vec in concept_vec_save:\n",
    "                        list_vec = list(vec)\n",
    "                        str_vec = \"\\t\".join([str(x) for x in list_vec[:-1]])\n",
    "                        file.write(str_vec + \"\\n\" + str(list_vec[-1]) + \"\\n\")\n",
    "        # self.training_instance_file.close()\n",
    "\n",
    "\n",
    "    def trainHLR(self, i, cut):\n",
    "        pr = 0.5\n",
    "        cur_fbr, cur_fbh, cur_fbt = self.D.fb_r[i], self.D.fb_h[i], self.D.fb_t[i]\n",
    "        if self.args[\"bern\"] == 1:\n",
    "            pr = float(self.D.right_num[cur_fbr]) / (self.D.right_num[cur_fbr] + self.D.left_num[cur_fbr])\n",
    "        if random.uniform(0, 1) < pr:\n",
    "            loop=True\n",
    "            while loop:\n",
    "\n",
    "                if len(self.D.instance_brother[cur_fbt]) > 0:\n",
    "                    if random.uniform(0, 1) < cut:\n",
    "                        j = random.randint(0, self.D.entity_num - 1)\n",
    "                    else:\n",
    "                        j = random.randint(0, len(self.D.instance_brother[cur_fbt]) - 1)\n",
    "                        j = self.D.instance_brother[cur_fbt][j]\n",
    "                else:\n",
    "                    j = random.randint(0, self.D.entity_num - 1)\n",
    "                loop = j in self.D.ok[(cur_fbh, cur_fbr)]\n",
    "            return cur_fbh, cur_fbt, cur_fbr, cur_fbh, j, cur_fbr\n",
    "        else:\n",
    "            loop=True\n",
    "            while loop:\n",
    "                if len(self.D.instance_brother[cur_fbh]) > 0:\n",
    "                    if random.uniform(0, 1) < cut:\n",
    "                        j = random.randint(0, self.D.entity_num - 1)\n",
    "                    else:\n",
    "                        j = random.randint(0, len(self.D.instance_brother[cur_fbh]) - 1)\n",
    "                        j = self.D.instance_brother[cur_fbh][j]\n",
    "                else:\n",
    "                    j = random.randint(0, self.D.entity_num - 1)\n",
    "                loop = ((j,cur_fbr) in self.D.ok) and (cur_fbt in self.D.ok[(j, cur_fbr)])\n",
    "            return cur_fbh, cur_fbt, cur_fbr, j, cur_fbt, cur_fbr\n",
    "\n",
    "    def trainInstanceOf(self, i, cut):\n",
    "        i = i - len(self.D.fb_h)\n",
    "        cur_ins,cur_cpt = self.D.instanceOf[i]\n",
    "        if random.randint(0, 1) == 0:\n",
    "            loop=True\n",
    "            while loop:\n",
    "                if len(self.D.instance_brother[cur_ins]) > 0: #\n",
    "                    if random.uniform(0, 1) < cut:\n",
    "                        j = random.randint(0, self.D.entity_num - 1)\n",
    "                    else:\n",
    "                        j = random.randint(0, len(self.D.instance_brother[cur_ins]) - 1)\n",
    "                        j = self.D.instance_brother[cur_ins][j]\n",
    "                else:\n",
    "                    j = random.randint(0, self.D.entity_num - 1)\n",
    "                loop = (j, cur_cpt) in self.D.instanceOf_ok\n",
    "            return cur_ins, cur_cpt, j, cur_cpt\n",
    "\n",
    "        else:\n",
    "            loop=True\n",
    "            while loop:\n",
    "                if len(self.D.concept_brother[cur_cpt]) > 0: #\n",
    "                    if random.uniform(0, 1) < cut:\n",
    "                        j = random.randint(0, self.D.concept_num - 1)\n",
    "                    else:\n",
    "                        j = random.randint(0, len(self.D.concept_brother[cur_cpt]) - 1)\n",
    "                        j = self.D.concept_brother[cur_cpt][j]\n",
    "                else:\n",
    "                    j = random.randint(0, self.D.concept_num - 1)\n",
    "                loop = (cur_ins, j) in self.D.instanceOf_ok\n",
    "            return cur_ins, cur_cpt, cur_ins, j\n",
    "\n",
    "    def trainSubClassOf(self, i, cut):\n",
    "        i = i - len(self.D.fb_h) - len(self.D.instanceOf)\n",
    "\n",
    "        cur_cpth,cur_cptt=self.D.subClassOf[i]\n",
    "        if random.randint(0, 1) == 0:\n",
    "            loop=True\n",
    "            while loop:\n",
    "                if len(self.D.concept_brother[cur_cpth]) > 0: #\n",
    "                    if random.uniform(0, 1) < cut:\n",
    "                        j = random.randint(0, self.D.concept_num - 1)\n",
    "                    else:\n",
    "                        j = random.randint(0, len(self.D.concept_brother[cur_cpth]) - 1)\n",
    "                        j = self.D.concept_brother[cur_cpth][j]\n",
    "                else:\n",
    "                    j = random.randint(0, self.D.concept_num - 1)\n",
    "                loop = (j, cur_cptt) in self.D.subClassOf_ok\n",
    "            return cur_cpth, cur_cptt, j, cur_cptt\n",
    "        else:\n",
    "            loop=True\n",
    "            while loop:\n",
    "                if len(self.D.concept_brother[cur_cptt]) > 0: #\n",
    "                    if random.uniform(0, 1) < cut:\n",
    "                        j = random.randint(0, self.D.concept_num - 1)\n",
    "                    else:\n",
    "                        j = random.randint(0, len(self.D.concept_brother[cur_cptt]) - 1)\n",
    "                        j = self.D.concept_brother[cur_cptt][j]\n",
    "                else:\n",
    "                    j = random.randint(0, self.D.concept_num - 1)\n",
    "                loop = (cur_cpth, j) in self.D.subClassOf_ok\n",
    "            return cur_cpth, cur_cptt, cur_cpth, j\n",
    "\n",
    "    def doTrainHLR(self, ids):\n",
    "        entity_embs = self.entity_vec(ids[[0, 1, 3, 4], :])\n",
    "        relation_embs = self.relation_vec(ids[[2, 5], :])\n",
    "\n",
    "        dis_a = norm(entity_embs[0] + relation_embs[0] - entity_embs[1],pnorm=self.args[\"pnorm\"])\n",
    "        dis_b = norm(entity_embs[2] + relation_embs[1] - entity_embs[3],pnorm=self.args[\"pnorm\"])\n",
    "\n",
    "        loss = F.relu(dis_a + self.args[\"margin_hrt\"] - dis_b).sum()\n",
    "        return loss,dis_a,dis_b\n",
    "\n",
    "    def doTrainInstanceOf(self, ids):\n",
    "        entity_embs = self.entity_vec(ids[[0, 2], :])\n",
    "        concept_embs = self.concept_vec(ids[[1, 3], :])\n",
    "        radius = concept_embs[:, :, -1]\n",
    "        concept_embs = concept_embs[:, :, :-1]\n",
    "\n",
    "        if self.args[\"pnorm\"]==1:\n",
    "            dis = F.relu(norm(entity_embs - concept_embs,pnorm=self.args[\"pnorm\"]) - torch.abs(radius))\n",
    "        else:\n",
    "            dis = F.relu(norm(entity_embs - concept_embs,pnorm=self.args[\"pnorm\"]) - radius ** 2)\n",
    "\n",
    "        loss = F.relu(dis[0] + self.args[\"margin_ins\"] - dis[1]).sum()\n",
    "        return loss\n",
    "\n",
    "    def doTrainSubClassOf(self, ids):\n",
    "        concept_embs_a = self.concept_vec(ids[[0,2],:])\n",
    "        concept_embs_b = self.concept_vec(ids[[1, 3], :])\n",
    "        radius_a = concept_embs_a[:, :, -1]\n",
    "        radius_b = concept_embs_b[:, :, -1]\n",
    "\n",
    "        concept_embs_a = concept_embs_a[:, :, :-1]\n",
    "        concept_embs_b = concept_embs_b[:, :, :-1]\n",
    "\n",
    "        if self.args[\"pnorm\"]==1:\n",
    "            dis = F.relu(norm(concept_embs_a - concept_embs_b,pnorm=self.args[\"pnorm\"]) + torch.abs(radius_a) - torch.abs(radius_b))\n",
    "        else:\n",
    "            dis = F.relu(norm(concept_embs_a - concept_embs_b,pnorm=self.args[\"pnorm\"]) + radius_a ** 2 - radius_b ** 2)\n",
    "\n",
    "        loss = F.relu(dis[0] + self.args[\"margin_sub\"] - dis[1]).sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(dataset,filename,split = 'Train'):\n",
    "    with open(\"data/\" + dataset + \"/\" + split+\"/\"+filename + \".txt\") as file:\n",
    "        L = file.readlines()\n",
    "        num = int(L[0].strip())\n",
    "        contents = [[x for x in line.strip().split()] for line in L[1:]]\n",
    "    return num, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_triples(dataset,filename,split = 'Train'):\n",
    "    with open(\"data/\" + dataset + \"/\" + split+\"/\"+filename + \".txt\") as file:\n",
    "        L = file.readlines()\n",
    "        num = int(L[0].strip())\n",
    "        contents = [[int(x) for x in line.strip().split()] for line in L[1:]]\n",
    "    return num, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_biples(dataset, filename,split = 'Train'):\n",
    "    with open(\"data/\" + dataset + \"/\" + split+\"/\"+filename + \".txt\") as file:\n",
    "        L = file.readlines()\n",
    "        contents = [[int(x) for x in line.strip().split()] for line in L[1:]]\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parseargs():\n",
    "#     parsers = argparse.ArgumentParser()\n",
    "#     parsers.add_argument(\"--emb_dim\", type=int, default=100)\n",
    "#     parsers.add_argument(\"--margin_hrt\", type=float, default=1.0)\n",
    "#     parsers.add_argument(\"--margin_ins\", type=float, default=0.4)\n",
    "#     parsers.add_argument(\"--margin_sub\", type=float, default=0.3)\n",
    "#     parsers.add_argument(\"--hrt_cut\", type=float, default=0.8)\n",
    "#     parsers.add_argument(\"--ins_cut\", type=float, default=0.8)\n",
    "#     parsers.add_argument(\"--sub_cut\", type=float, default=0.8)\n",
    "\n",
    "#     parsers.add_argument(\"--nepoch\", type=float, default=1000)\n",
    "#     parsers.add_argument(\"--nbatches\", type=float, default=100)\n",
    "\n",
    "#     parsers.add_argument(\"--lr\", type=float, default=0.001)\n",
    "#     parsers.add_argument(\"--bern\", type=int, default=1)\n",
    "#     parsers.add_argument(\"--pnorm\", type=int, default=1)\n",
    "#     parsers.add_argument(\"--dataset\", type=str, default=\"YAGO39K\")\n",
    "#     parsers.add_argument(\"--split\", type=str, default=\"Train\")\n",
    "#     parsers.add_argument(\"--version\", type=str, default='tmp')\n",
    "\n",
    "#     args = parsers.parse_args()\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(config=None):\n",
    "    if config is None:\n",
    "        config = {\n",
    "            \"emb_dim\": 100,\n",
    "            \"margin_hrt\": 1.0,\n",
    "            \"margin_ins\": 0.4,\n",
    "            \"margin_sub\": 0.3,\n",
    "            \"hrt_cut\": 0.8,\n",
    "            \"ins_cut\": 0.8,\n",
    "            \"sub_cut\": 0.8,\n",
    "            \"nepoch\": 1000,\n",
    "            \"nbatches\": 100,\n",
    "            \"lr\": 0.001,\n",
    "            \"bern\": 1,\n",
    "            \"pnorm\": 1,\n",
    "            \"dataset\": \"YAGO39K\",\n",
    "            \"split\": \"Train\",\n",
    "            \"version\": \"tmp\",\n",
    "        }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     args = parseargs()\n",
    "\n",
    "#     if not os.path.exists(\"data/\" + args.dataset + \"/\" + args.split + \"/processed.pkl\"):\n",
    "#         dataset = Dataset(args=args)\n",
    "#         dataset.setup()\n",
    "#         dataset.save()\n",
    "#     else:\n",
    "#         dataset = load_processed(dataset_name=args.dataset, split=args.split)\n",
    "#         print(\"dataset loaded\")\n",
    "\n",
    "#     train = Train(args = args,dataset= dataset).cuda()\n",
    "#     train.doTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config=None):\n",
    "    if config is None:\n",
    "        config = parse_args()\n",
    "\n",
    "    if not os.path.exists(\"data/\" + config[\"dataset\"] + \"/\" + config[\"split\"] + \"/processed.pkl\"):\n",
    "        dataset = Dataset(args=config)\n",
    "        dataset.setup()\n",
    "        dataset.save()\n",
    "    else:\n",
    "        dataset = load_processed(dataset_name=config[\"dataset\"], split=config[\"split\"])\n",
    "        print(\"dataset loaded\")\n",
    "\n",
    "    train = Train(args=config, dataset=dataset).cuda()\n",
    "    train.doTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11660\\451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11660\\871364541.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m         \"\"\"\n\u001b[1;32m--> 918\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m                 \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m         \"\"\"\n\u001b[1;32m--> 918\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    287\u001b[0m             )\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cuda_getDeviceCount\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
